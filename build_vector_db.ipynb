{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73098c5",
   "metadata": {},
   "source": [
    "# Part A: Chroma Vector Database Ingestion\n",
    "\n",
    "Set up a local Chroma store that the TTD-DR agent can reuse for feasibility-research retrieval. **Do not run the cells yet**‚Äîwe will swap the dummy source links for production data before ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed99ba3f",
   "metadata": {},
   "source": [
    "## Notebook Goals\n",
    "- Import the minimal tooling for Chroma + embeddings\n",
    "- Define reusable configuration plus placeholder (HTML/JSON) data sources\n",
    "- Fetch, clean, and chunk remote text; PDF ingestion is intentionally removed\n",
    "- Persist a Chroma collection and a lightweight pickle manifest for downstream agents\n",
    "- Provide clear entry points to replace dummy URLs with the real parcel intelligence feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a68033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports ready (execute once sources are finalized)\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports & Environment (run only after providing real source URLs)\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from uuid import uuid4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "load_dotenv()\n",
    "print(\"‚úÖ Imports ready (execute once sources are finalized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab4041",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Define paths and parameters. Sources will be loaded from YAML in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ee8629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Persist directory: data/vectorstores/chroma_feasibility\n",
      "üóÇÔ∏è Manifest path: data/vectorstores/chroma_manifest.pkl\n",
      "üìÑ Sources file: data/sources.yaml\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "VECTOR_DIR = DATA_DIR / \"vectorstores\" / \"chroma_feasibility\"\n",
    "MANIFEST_PATH = DATA_DIR / \"vectorstores\" / \"chroma_manifest.pkl\"\n",
    "SOURCES_FILE = DATA_DIR / \"sources.yaml\"\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "COLLECTION_NAME = \"ttd_dr_feasibility_seed\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "REQUEST_TIMEOUT = 30\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÇ Persist directory: {VECTOR_DIR}\")\n",
    "print(f\"üóÇÔ∏è Manifest path: {MANIFEST_PATH}\")\n",
    "print(f\"üìÑ Sources file: {SOURCES_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77860e91",
   "metadata": {},
   "source": [
    "## 3. Load Sources from YAML\n",
    "Load the source list from `data/sources.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72e7bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loaded 15 sources from data/sources.yaml\n",
      "  - Los Angeles County Assessor Portal [html] -> https://portal.assessor.lacounty.gov/\n",
      "  - USGS The National Map [html] -> https://www.usgs.gov/programs/national-geospatial-program/national-map\n",
      "  - OpenStreetMap Land Use & POIs [html] -> https://wiki.openstreetmap.org/wiki/Map_features\n",
      "  - NYC Zoning Resolution Portal [html] -> https://zr.planning.nyc.gov/\n",
      "  - US Census TIGER/Line Overview [html] -> https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html\n",
      "  - FEMA Flood Map Service Center [html] -> https://msc.fema.gov/portal/home\n",
      "  - USDA Web Soil Survey [html] -> https://websoilsurvey.sc.egov.usda.gov/App/HomePage.htm\n",
      "  - EPA Envirofacts [html] -> https://enviro.epa.gov/\n",
      "  - EPA EJScreen Portal [html] -> https://www.epa.gov/ejscreen\n",
      "  - Transit.land National Transit Map [html] -> https://www.transit.land/\n",
      "  - OpenAddresses Global Address Data [html] -> https://openaddresses.io/\n",
      "  - US Census ACS Data Portal [html] -> https://data.census.gov/cedsci/\n",
      "  - Bureau of Labor Statistics Regional Data [html] -> https://www.bls.gov/regions/home.htm\n",
      "  - Zillow Research Hub [html] -> https://www.zillow.com/research/data/\n",
      "  - TTD-DR Paper (arXiv abstract) [html] -> https://arxiv.org/abs/2507.16075\n"
     ]
    }
   ],
   "source": [
    "# 3. Load Sources from YAML\n",
    "import yaml\n",
    "\n",
    "def load_sources(path: Path) -> List[Dict[str, str]]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Sources file not found at {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f) or []\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Expected a list of sources in YAML\")\n",
    "    return data\n",
    "\n",
    "SOURCES = load_sources(SOURCES_FILE)\n",
    "print(f\"üìö Loaded {len(SOURCES)} sources from {SOURCES_FILE}\")\n",
    "for src in SOURCES:\n",
    "    print(f\"  - {src.get('name', 'Unnamed Source')} [{src.get('type', 'unknown')}] -> {src.get('url')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c39a6",
   "metadata": {},
   "source": [
    "## 3. Source Fetching Helpers (HTML & JSON)\n",
    "These utilities intentionally skip PDF/file downloads. Replace the dummy URLs with real feeds before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff4c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url: str) -> str:\n",
    "    response = requests.get(url, timeout=REQUEST_TIMEOUT)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.extract()\n",
    "    text = \" \".join(chunk.strip() for chunk in soup.stripped_strings)\n",
    "    return text\n",
    "\n",
    "\n",
    "def fetch_json(url: str) -> str:\n",
    "    response = requests.get(url, timeout=REQUEST_TIMEOUT)\n",
    "    response.raise_for_status()\n",
    "    payload = response.json()\n",
    "    return json.dumps(payload, indent=2)\n",
    "\n",
    "\n",
    "def build_documents(sources: List[Dict[str, str]]) -> List[Document]:\n",
    "    documents = []\n",
    "    for source in sources:\n",
    "        try:\n",
    "            if source[\"type\"].lower() == \"html\":\n",
    "                raw_text = fetch_html(source[\"url\"])\n",
    "            elif source[\"type\"].lower() == \"json\":\n",
    "                raw_text = fetch_json(source[\"url\"])\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping unsupported type: {source['type']} for {source['name']}\")\n",
    "                continue\n",
    "\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=raw_text,\n",
    "                    metadata={\n",
    "                        \"source\": source[\"url\"],\n",
    "                        \"name\": source[\"name\"],\n",
    "                        \"notes\": source.get(\"notes\", \"\"),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            print(f\"‚úÖ Loaded {source['name']}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"‚ùå Failed to ingest {source['name']}: {exc}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d7a92",
   "metadata": {},
   "source": [
    "## 4. Chunk Strategy\n",
    "Configure a `RecursiveCharacterTextSplitter` so long municipal reports or API payloads become retrieval-friendly passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d9b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x1405cd4f0>\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    ")\n",
    "print(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e566fe49",
   "metadata": {},
   "source": [
    "## 5. Initialize Embeddings + Chroma\n",
    "Instantiate OpenAI embeddings and a persistent Chroma collection. Swap providers or deployment modes as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba139d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Chroma collection ready: ttd_dr_feasibility_seed\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "vector_store = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=str(VECTOR_DIR),\n",
    ")\n",
    "print(f\"üìö Chroma collection ready: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9a913",
   "metadata": {},
   "source": [
    "## 6. Build Documents from YAML Sources\n",
    "Load and process documents from the sources defined in `data/sources.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a893c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded Los Angeles County Assessor Portal\n",
      "‚úÖ Loaded USGS The National Map\n",
      "‚úÖ Loaded OpenStreetMap Land Use & POIs\n",
      "‚úÖ Loaded NYC Zoning Resolution Portal\n",
      "‚úÖ Loaded US Census TIGER/Line Overview\n",
      "‚ùå Failed to ingest FEMA Flood Map Service Center: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "‚úÖ Loaded USDA Web Soil Survey\n",
      "‚úÖ Loaded EPA Envirofacts\n",
      "‚ùå Failed to ingest EPA EJScreen Portal: 404 Client Error: Not Found for url: https://www.epa.gov/ejscreen\n",
      "‚úÖ Loaded Transit.land National Transit Map\n",
      "‚úÖ Loaded OpenAddresses Global Address Data\n",
      "‚úÖ Loaded US Census ACS Data Portal\n",
      "‚ùå Failed to ingest Bureau of Labor Statistics Regional Data: 403 Client Error: Forbidden for url: https://www.bls.gov/regions/home.htm\n",
      "‚ùå Failed to ingest Zillow Research Hub: 403 Client Error: Forbidden for url: https://www.zillow.com/research/data/\n",
      "‚úÖ Loaded TTD-DR Paper (arXiv abstract)\n",
      "Total raw documents: 11\n"
     ]
    }
   ],
   "source": [
    "raw_documents = build_documents(SOURCES)\n",
    "print(f\"Total raw documents: {len(raw_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113119a",
   "metadata": {},
   "source": [
    "## 7. Chunk & Upsert into Chroma\n",
    "Split the raw payloads, add them to the vector store, and persist the collection to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71457fcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'persist'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     ids = [\u001b[38;5;28mstr\u001b[39m(uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m chunked_documents]\n\u001b[32m      7\u001b[39m     vector_store.add_documents(documents=chunked_documents, ids=ids)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpersist\u001b[49m()\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Stored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunked_documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'Chroma' object has no attribute 'persist'"
     ]
    }
   ],
   "source": [
    "chunked_documents = []\n",
    "for doc in raw_documents:\n",
    "    chunked_documents.extend(text_splitter.split_documents([doc]))\n",
    "\n",
    "if chunked_documents:\n",
    "    ids = [str(uuid4()) for _ in chunked_documents]\n",
    "    vector_store.add_documents(documents=chunked_documents, ids=ids)\n",
    "    vector_store.persist()\n",
    "    print(f\"‚úÖ Stored {len(chunked_documents)} chunks in {COLLECTION_NAME}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents were ingested. Update DUMMY_SOURCES and rerun.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e403a",
   "metadata": {},
   "source": [
    "## 8. Persist Retrieval Manifest (Pickle)\n",
    "The manifest lets downstream agents reconnect to the same Chroma index without re-embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b51d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_manifest(documents: List[Document]) -> Dict:\n",
    "    summary = []\n",
    "    for doc in documents:\n",
    "        summary.append(\n",
    "            {\n",
    "                \"source\": doc.metadata.get(\"source\"),\n",
    "                \"name\": doc.metadata.get(\"name\"),\n",
    "                \"char_count\": len(doc.page_content),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    manifest = {\n",
    "        \"collection_name\": COLLECTION_NAME,\n",
    "        \"persist_directory\": str(VECTOR_DIR),\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"document_summary\": summary,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "        \"generated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    return manifest\n",
    "\n",
    "\n",
    "if raw_documents:\n",
    "    manifest = generate_manifest(raw_documents)\n",
    "    with MANIFEST_PATH.open(\"wb\") as f:\n",
    "        pickle.dump(manifest, f)\n",
    "    print(f\"üìù Manifest saved to {MANIFEST_PATH}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Manifest not written because no documents were ingested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43840bae",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "1. Replace the dummy URLs with the real address/parcel research links you will provide.\n",
    "2. Run the notebook top-to-bottom to ingest and persist the data.\n",
    "3. Load `chroma_manifest.pkl` inside the retrieval layer to avoid re-embedding.\n",
    "4. Version the `data/vectorstores` directory (or sync to object storage) so every agent run can mount the same context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
